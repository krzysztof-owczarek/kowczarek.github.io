# Lessons learned: How turning on Horizontal Pod Autoscaler on the perfectly balanced K8s deployment can degrade the performence by more thatn 50%.

Recently, I have been working on a proper K8s deployment setup for an application that has been designed to handle a very heavy traffic spikes. Those traffic are happening from time to time and are essential for our business, so it has been absolutely crucial to be able to handle all the incoming traffic without **any** service degradation.

When you work on spikes of synchronously processed traffic only, you do not get second chances.

## Prepare your toolset

TODO

## Get to know your cluster well

1. Unless you are a DevOPs, SRE or any other Kubernates specialist that has been around while the cluster has been set up, you probably do not have a clue how it has been configured and in bigger organisations it may be quite hard to find that out. If you have good monitoring you can probably deduce some things about the cluster, how much nodes are up and how many resources they use, but you will probably not know what are the scaling strategies, what is the resource cap on the whole cluster if any etc.

2. When making crucial deployment decisions such as resource allocation, initial replicas count or HPA scaling strategy experiment a bit, test it out on the scenario valid for your case and do not settle for the first good result.

3. Some clusters behave better if you scale out to many small replicas, some will favour a few bigger ones. I am not expert on K8s configuration, but if your HPA will trigger a cluster scaling and you go for no traffic degradation you will get weak results. 

## OOMKiller does not always work perfectly, CPU overcommittment happens and it is not always easy to notice

It might be a configuration thing, but I have seen a lot of pods that were trying to use well over their memory assignment value and lived long enough to disrupt 






## General overview of the request processing

Response for each call has been a composition of successfully ended integration calls that happen synchronously one after another. Integration calls involve various technologies that utilize different phisical resources, stacks, layers and pools you have to take into consideration, while assigning resources to the application pods.

```
Examples:
- database (connections in the pool, transaction management)
- REST endpoints (server threads, request/socket timeouts),
- gRPC services (gRPC service thread pools, clients and server timeouts and deadlines),
- ...
```

[memory]
Aformentioned call are performed to the external services and each has a noticable *response latency*. During the time needed for a completion of all subsequent, synchronous requests most of intermidiate objects have to live in memory generating a noticable **memory footprint**, which will have to be cleaned up by **Garbage Collector** some time.

**Note: choosing a right GC that will provide a performant cleaning strategy for your scenario, while keeping latency and CPU usage low is very important, but outside the scope of this article ** 

[cpu]
All the integrations need some part of your assigned CPU time. Garbage collector will use it too. Unless you plan to assign a lot of CPU time to your deployment and than divide this value into small, but predictable *fixed size thread pools* you don't have to worry much about **throttling** and **context switching**. 

If you want to achieve a good throughput though, you have to make your code ready for heavy context switching and get rid of everything that would block or pin threads like state mutation, synchronized blocks etc. Enabling Virtual Threads will boost your performence, but **throttling** becomes your concern. Enabling Virtual Threads may add additional memory footprint too.

# Introducing HPA (Horizontal Pod Autoscaler) to the well balanced deployment

The deployment has been balanced. I have found the right resource assignment per pod and a minimal number of replicas, that were sufficient enought to meet all of my test requirements.

It worked great, but as I have said before the overprovisioning was not warmly welcome. It is easy and expensive to just overprovision without thinking. It is usually overused in the industry to hide problems with the applications by covering it with some money too.

Of course there are usecases where the cost of a reasonable overprovisioning is well justified, but our production scenario was mainly about the peaks of traffic during some busy hours when most of the business happened. For the rest of the time (days or even weeks) the deployment would be mostly idle, so there was a huge place for an improvement to introduce a proper HPA and save a lot of money on resources and replicas.

**Starting strong and failing immidiately**

The first thing I have tried was just to enable HPA, setup a low CPU usage treshold for scaling up some additional instances and run the test again. I have not expected any trouble as the initial setup has been perfectly capable of sustaining the load generated by the Gatling scenario. I was very suprised when I have found out that the moment HPA starts to scale up instances everything slows down to the point where k8s healthcheck probes on pods time out and the whole deployment locks in the rollout restarts resulting in around 30-50% of initial performence.

**Long story short**

It appeared that each time the HPA started to scale out the new pods the CPU requested value for the deployment went over the CPU assigned metric.





Turning on HPA (Horizotal Pod Autoscaler) on well balanced (resource and replica-wise), non-HPA Kubernetes deployment of a heavy traffic app ruins everything.

**Long Problem Description**

Having a Gatling scenario that performs a a constant, heavy traffic load test on the CPU intensive application (see scenario shape on diagram no. 1) I have managed to balance minimal resources and replicas that are able to handle the load test without any problems. The appliction in test should be able to handle such a heavy traffic at any time for any duration necessary.

The app and the deployments needs to handle such spikes, but on production, such heavy traffic happends only a few times a month. For the rest of time the application stays close to IDLE, so the natural next step to take was to start with only a few replicas and scale horizontaly when needed.




DRAFT

I have been recently working on introducing a HPA on a k8s deployment of an application that:
- needs to handle a heavy, constant traffic for a long time,
- is CPU intensive,
- is CPU throttling sensitive,
- needs full power very rarely, so most of the time it should not use many resources and should not be scaled to many replicas.

I have created a Gatling scenario that I have been using to test the app.
Load shape looks like that: <insert>

Before starting to solve the HPA problem, I have already found an thouroughly load tested a minimal static (no HPA) balance between resources and replias, so the natural first step has been starting with the same resource assigments, but use less replicas on start and try to find a HPA setting that will make the test pass.

... and there is where problems started. It just did not work.

I have took a step back, set up HPA that started with a sufficient replicas to handle the whole load and just add replicas extra. It did not work!

The setup that has been previously and statically sufficient to handle everything started to work horribly. Integration calls via gRPC and HTTP started to take longer and longer, CPU throttling has started to show up, and CPU load peaked. In effect the k8s probes on the apps had started to lag so much, k8s has been restarting them putting even more straing on the remaining pods that were slower and slower until restarted too.

There has been a suspision that a k8s cluster is scaling out nodes or available resources underneath when HPA kicks in, but it was quite hard to pin point what is going on and as for some of you that are more expierienced in the k8s mechanics some metrics and graphs revealing the problem might pop up in minds it has not been easy to find for me, not my SRE contacts I have tried to consult on the matter.

Break through.
I have already monitored CPU and Memory assignments used/assigned for the pods, but I have not came up with the possible solution until two things happaned.

- I have seen the same cpu/mem graphs assigned/used, but plotted differently (how important it can be!) - they were not just lines, they were lined with filling underneath plotted on the same graph!

- I have started to extensively search more and more information about recource assignments to understand how k8s can use more cpu or mem that it was assign to the pod and were it breaks and found a "do not use a CPU limit on k8s" article.

--- 

what the graphs were saying?
pods  were using much more cpu than assigned to pods in deployment and they seem to be crushed for that by a killer process

- jvm warm ups
- how to compensate for that additional usage for a short period of time?

Article points out that when you do not set up limit on CPU assignment you might get any unused CPU cycles if there is any on the node - that has changed everything!

TBC
