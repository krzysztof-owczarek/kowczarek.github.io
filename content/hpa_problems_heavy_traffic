**Short problem Review no.1**

Recently, I have been working on setting up a proper deployment for an application that has been designed to handle a very heavy traffic spikes. Those traffic spikes were essential for our business and it has been absolutely crucial to handle all the incoming requests without any service degradation.

I have decided to write a simple Gatling scenario, that would perform calls to the API with a configurable RPS for a configurable time duration. The shape of the scenario looked more-less like this:

<insert>

Each call to the API has been starting a chain of integration calls happening mostly synchronously:
- calls to database,
- calls to REST endpoints,
- calls to gRPC services,
- producing messages on Kafka,
- etc.

As such request/response roundtrips take time you can imagine it has not only generated a noticable (not big though) *memory* footprint (that GC had to clean up quite often also using CPU cycles for that matter), but it has also depended on *CPUs* with a lot of incoming requests and a great amount of *context switching*. Deployment had to be well balanced to have all the resources and replicas in place to let the application do its job, keeping in mind that overprovision (horizontal and vertical) is not welcome.

**Introducing HPA (Horizontal Pod Autoscaler) to the well balanced deployment**

The deployment has been balanced. I have found the right resource assignment per pod and a minimal number of replicas, that were sufficient enought to meet all of my test requirements.

It worked great, but as I have said before the overprovisioning was not warmly welcome. It is easy and expensive to just overprovision without thinking. It is usually overused in the industry to hide problems with the applications by covering it with some money too.

Of course there are usecases where the cost of a reasonable overprovisioning is well justified, but our production scenario was mainly about the peaks of traffic during some busy hours when most of the business happened. For the rest of the time (days or even weeks) the deployment would be mostly idle, so there was a huge place for an improvement to introduce a proper HPA and save a lot of money on resources and replicas.

**Starting strong and failing immidiately**

The first thing I have tried was just to enable HPA, setup a low CPU usage treshold for scaling up some additional instances and run the test again. I have not expected any trouble as the initial setup has been perfectly capable of sustaining the load generated by the Gatling scenario. I was very suprised when I have found out that the moment HPA starts to scale up instances everything slows down to the point where k8s healthcheck probes on pods time out and the whole deployment locks in the rollout restarts resulting in around 30-50% of initial performence.

**Long story short**

It appeared that each time the HPA started to scale out the new pods the CPU requested value for the deployment went over the CPU assigned metric.





Turning on HPA (Horizotal Pod Autoscaler) on well balanced (resource and replica-wise), non-HPA Kubernetes deployment of a heavy traffic app ruins everything.

**Long Problem Description**

Having a Gatling scenario that performs a a constant, heavy traffic load test on the CPU intensive application (see scenario shape on diagram no. 1) I have managed to balance minimal resources and replicas that are able to handle the load test without any problems. The appliction in test should be able to handle such a heavy traffic at any time for any duration necessary.

The app and the deployments needs to handle such spikes, but on production, such heavy traffic happends only a few times a month. For the rest of time the application stays close to IDLE, so the natural next step to take was to start with only a few replicas and scale horizontaly when needed.




DRAFT

I have been recently working on introducing a HPA on a k8s deployment of an application that:
- needs to handle a heavy, constant traffic for a long time,
- is CPU intensive,
- is CPU throttling sensitive,
- needs full power very rarely, so most of the time it should not use many resources and should not be scaled to many replicas.

I have created a Gatling scenario that I have been using to test the app.
Load shape looks like that: <insert>

Before starting to solve the HPA problem, I have already found an thouroughly load tested a minimal static (no HPA) balance between resources and replias, so the natural first step has been starting with the same resource assigments, but use less replicas on start and try to find a HPA setting that will make the test pass.

... and there is where problems started. It just did not work.

I have took a step back, set up HPA that started with a sufficient replicas to handle the whole load and just add replicas extra. It did not work!

The setup that has been previously and statically sufficient to handle everything started to work horribly. Integration calls via gRPC and HTTP started to take longer and longer, CPU throttling has started to show up, and CPU load peaked. In effect the k8s probes on the apps had started to lag so much, k8s has been restarting them putting even more straing on the remaining pods that were slower and slower until restarted too.

There has been a suspision that a k8s cluster is scaling out nodes or available resources underneath when HPA kicks in, but it was quite hard to pin point what is going on and as for some of you that are more expierienced in the k8s mechanics some metrics and graphs revealing the problem might pop up in minds it has not been easy to find for me, not my SRE contacts I have tried to consult on the matter.

Break through.
I have already monitored CPU and Memory assignments used/assigned for the pods, but I have not came up with the possible solution until two things happaned.

- I have seen the same cpu/mem graphs assigned/used, but plotted differently (how important it can be!) - they were not just lines, they were lined with filling underneath plotted on the same graph!

- I have started to extensively search more and more information about recource assignments to understand how k8s can use more cpu or mem that it was assign to the pod and were it breaks and found a "do not use a CPU limit on k8s" article.

--- 

what the graphs were saying?
pods  were using much more cpu than assigned to pods in deployment and they seem to be crushed for that by a killer process

- jvm warm ups
- how to compensate for that additional usage for a short period of time?

Article points out that when you do not set up limit on CPU assignment you might get any unused CPU cycles if there is any on the node - that has changed everything!

TBC
