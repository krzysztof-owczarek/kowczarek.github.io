# Lessons learned: finding the right deployment configuration for your traffic intensive app and how turning on HPA can degrade the initial performence by more that 50%.

Recently, I have been working on a proper K8s deployment setup for an application that has been designed to handle big traffic spikes. Those spikes are rare, but when appear, the deployment has to be ready to handle them. Designing and building an app and the architecture that will work with sychronous traffic spikes is tricky as you usually don't get any second chances.

## Finding the configuration that will not overprovision resources and replicas is not easy, so you need a good tool.

Use one of the load test tools for putting a strain on crucial parts of your system or build a custom one. It can be a short Gatling scenario that would be easy to run against your test cluster as many times as you need, whenever you need.

**hint:** if your K8s cluster is hosted on the cloud like AWS, it is usually a good idea to run the load test from **within the cluster** as cloud providers tend to charge for incoming **external** traffic.

**hint2:** it might be a good idea to isolate your app from the external downstreams/upstreams with mocks and first make sure that your codebase performance is high enough. It will be much easier to eliminate performance bottlenecks in your code first.

**hint3:** when you mock external services, introduce artificial latencies. Do not settle for a fixed one, it is very easy to add a small variance (random latency between range of X and Y) and such a scenario will be much closer to the real thing. Try to determine what is an average response time of the service you mock and use that value as a mean value for your random artificial latency.

**hint4:** when your codebase pass your load test scenario remove integration mocks one by one. Run load tests after introducing every real integration to check if it is still passing.

**hint5:** experiment with different configurations. For instance, if your application has been written in Java, try to run it on a bigger number of small JVMs or a small number of big JVMs. Do not hesitate to try out various Garbage Collectors to find the one that suits your needs. Not every aspect of optimisation is easy to forsee, some things have to be found experimentally.

**hint6:** if you find the perfect minimal configuration that make your load scenario(s) pass, consider adding 1 or 2 replicas more just for backup as K8s does not guarantee that it won't kill your pods from time to time while freeing up some resources or after adding new nodes to the cluster.

## Get to know your cluster well

1. Unless you are a DevOPs, SRE or any other Kubernates specialist that has been around while the cluster has been set up, you probably do not have a clue how it has been configured. In bigger organisations it may be quite hard to find that out too.

If you have a good monitoring you can probably learn a few things about the cluster, for example, how much nodes are currently up or how many resources it uses, but you will probably not know what are the scaling strategies, what is the resource cap on the whole cluster etc.

2. When making crucial deployment decisions such as:
- resource allocation per pod,
- initial replicas count,
- or HPA scaling strategy

experiment a bit. Test it out on the scenario that is valid for your case and do not settle for the first good result.

3. Some clusters behave better if you use many small replicas, some will favour a few bigger ones. I am not expert on K8s configuration, but my tests show that if your HPA triggers a cluster rescaling while you load test the application you will get bed results.

## OOMKiller does not always work perfectly

It might be a configuration thing, but I have seen a lot of pods that were trying to use well over their memory assignment value and lived long enough to disrupt the whole deployment's performence.

## CPU throttling is a silent killer

When your application starts to overuse assign CPU time you will face throttling. Throttling is much harder to pinpoint than memory problems, because the application may live long enough to produce errors that will cover the real issue.

Some important observations while load testing a CPU intensive JVM application:
- your CPU usage will be much higher on the fresh pods due to the JVM warmup, you have take this into consideration or you will quickly face pod kills and restarts from the cluster,
- throttling does not make the service blow up at once. The service will usually work for some time, degrading gradually while stedy flow of requests will make CPU switch context frequently having very little time to spend for each operation. In effect the response times will grow higher to the point your service starts to hit timeout somewhere,
- after some time your k8s probes start to timeout too, so the cluster will begin to restart your pods. Traffic that was flowing through killed pods a second ago will hit remaining pods making them throttle even quicker,
- if you are using any reverse proxy sidecars like Istio its CPU and memory usage can go off charts too. Another thing that can distract you from the real problem.
